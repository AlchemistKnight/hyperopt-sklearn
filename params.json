{"name":"Hyperopt-sklearn","tagline":"Hyper-parameter optimization for sklearn","body":"### Work In Progress...\r\n\r\n### Usage\r\n\r\n\r\n\r\n### Search Algorithms\r\n\r\n\r\n### Classifiers\r\n\r\nClassifiers from sklearn that are built-in so far:\r\n\r\nSVC\r\nLinearSVC\r\nKNeightborsClassifier\r\nRandomForestClassifier\r\nExtraTreesClassifier\r\nSGDClassifier\r\nMultinomialNB\r\nBernoulliRBM\r\nColumnKMeans\r\n\r\nMore to come!\r\n\r\n### Preprocessing\r\n\r\nPreprocessing steps from sklearn that are built-in so far:\r\n\r\nPCA\r\nTfidfVectorizer\r\nStandardScalar\r\nMinMaxScalar\r\nNormalizer\r\nOneHotEncoder\r\n\r\n\r\nMore to come!\r\n\r\n### Installation\r\n\r\n\r\n### Documentation\r\n\r\n\r\n### Examples\r\n\r\n\r\nput in examples with:\r\nany_classifier\r\nany_sparse_classifier\r\n\r\nAn example on the MNIST digit data\r\n```python\r\n\tfrom hpsklearn import hyperopt_estimator, any_classifier\r\n\tfrom sklearn.datasets import fetch_mldata\r\n\tfrom sklearn import metrics\r\n\tfrom hyperopt import tpe\r\n\timport numpy as np\r\n\t\r\n\t# Download the data and split into training and test sets\r\n\t\r\n\tdigits = fetch_mldata('MNIST original')\r\n\t\r\n\tX = digits.data\r\n\ty = digits.target\r\n\t\r\n\ttest_size = int( 0.2 * len( y ) )\r\n\tnp.random.seed( seed )\r\n\tindices = np.random.permutation(len(X))\r\n\tX_train = X[ indices[:-test_size]]\r\n\ty_train = y[ indices[:-test_size]]\r\n\tX_test = X[ indices[-test_size:]]\r\n\ty_test = y[ indices[-test_size:]]\r\n\t\r\n\testim = hyperopt_estimator( classifier=any_classifier('clf'),  \r\n\t\t\t\talgo=tpe.suggest, trial_timeout=300)\r\n\t\r\n\testim.fit( X_train, y_train )\r\n\t\r\n\tpred = estim.predict( X_test )\r\n\t\r\n\tprint( metrics.f1_score( pred, y_test ) )\r\n\t# <<show score here>>\r\n\tprint( estim.best_model() )\r\n\t# <<show model here>>\r\n```\r\nNot all classifiers within sklearn support sparse data. To make life easier, hpsklearn comes with an any_sparse_classifier which will only sample from the classifiers available which accept sparse data.\r\n\r\n```python\r\n\tfrom hpsklearn import hyperopt_estimator, any_sparse_classifier, tfidf\r\n\tfrom sklearn.datasets import fetch_20newsgroups\r\n\tfrom sklearn import metrics\r\n\tfrom hyperopt import tpe\r\n\timport numpy as np\r\n\t\r\n\t# Download the data and split into training and test sets\r\n\t\r\n\ttrain = fetch_20newsgroups( subset='train' )\r\n\ttest = fetch_20newsgroups( subset='test' )\r\n\t X_train = train.data\r\n\ty_train = train.target\r\n\tX_test = test.data\r\n\ty_test = test.target\r\n\t\r\n\testim = hyperopt_estimator( classifier=any_sparse_classifier('clf'),  preprocessing=[tfidf('tfidf')], \r\n\t\t\t\talgo=tpe.suggest, trial_timeout=300)\r\n\t\r\n\testim.fit( X_train, y_train )\r\n\t\r\n\tpred = estim.predict( X_test )\r\n\t\r\n\tprint( metrics.f1_score( pred, y_test ) )\r\n\t# <<show score here>>\r\n\tprint( estim.best_model() )\r\n\t# <<show model here>>\r\n```","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}