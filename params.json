{"name":"Hyperopt-sklearn","tagline":"Hyper-parameter optimization for sklearn","body":"## What is Hyperopt-sklearn?\r\n\r\nFinding the right classifier to use for your data can be hard. Once you have chosen a classifier, tuning all of the parameters to get the best results is tedious and time consuming. Even after all of your hard work, you may have chosen the wrong classifier to begin with. Hyperopt-sklearn provides a solution to this problem.\r\n\r\n## Usage\r\n\r\n```python\r\nfrom hpsklearn import hyperopt_estimator\r\n\r\n# Load Data\r\n# ...\r\n\r\n# Create the estimator object\r\nestim = hyperopt_estimator()\r\n\r\n# Search the space of classifiers and preprocessing steps and their\r\n# respective hyperparameters in sklearn to fit a model to the data\r\nestim.fit( train_data, train_label )\r\n\r\n# Make a prediction using the optimized model\r\nprediction = estim.predict( unknown_data )\r\n\r\n# Report the accuracy of the classifier on a given set of data\r\nscore = estim.score( test_data, test_label )\r\n\r\n# Return instances of the classifier and preprocessing steps\r\nmodel = estim.best_model()\r\n```\r\n\r\n## Search Algorithms\r\n\r\nAny search algorithm available in hyperopt can be used to drive the estimator. It is also possible to supply your own or use a mix of algorithms. The number of points to evaluate before returning, as well as an optional timeout (in seconds) can be used with any search algorithm. \r\n\r\n```python\r\nfrom hpsklearn import hyperopt_estimator\r\nfrom hyperopt import tpe\r\n\r\nestim = hyperopt_estimator( algo=tpe.suggest, \r\n                            max_evals=150, \r\n                            trial_timeout=60 )\r\n```\r\n\r\nSearch algorithms available so far:\r\n\r\n* Random Search\r\n* Tree of Parzen Estimators (TPE)\r\n* Annealing\r\n* Tree\r\n* Gaussian Process Tree\r\n\r\n## Classifiers\r\n\r\nIf you know what type of classifier you wish to use on your dataset, you can let hpsklearn know and it will only search in the parameter space of the given classifier.\r\n\r\n```python\r\nfrom hpsklearn import hyperopt_estimator, svc\r\n\r\nestim = hyperopt_estimator( classifier=svc('mySVC') )\r\n```\r\n\r\nYou can also provide sets of classifiers, and optionally choose the probability of the estimator picking each one.\r\n\r\n```python\r\nfrom hpsklearn import hyperopt_estimator, random_forest, svc, knn\r\nfrom hyperopt import hp\r\n\r\nclf = hp.pchoice( 'my_name', \r\n          [ ( 0.4, random_forest('my_name.random_forest') ),\r\n            ( 0.3, svc('my_name.svc') ),\r\n            ( 0.3, knn('my_name.knn') ) ]\r\n\r\nestim = hyperopt_estimator( classifier=clf )\r\n```\r\n\r\nClassifiers from sklearn that are built-in so far:\r\n\r\n* SVC\r\n* LinearSVC\r\n* KNeightborsClassifier\r\n* RandomForestClassifier\r\n* ExtraTreesClassifier\r\n* SGDClassifier\r\n* MultinomialNB\r\n* BernoulliRBM\r\n* ColumnKMeans\r\n\r\nMore to come!\r\n\r\n## Preprocessing\r\n\r\nYou also have control over which preprocessing steps are applied to your data. These can be passed as a list to the hyperopt_estimator. A blank list indicates no preprocessing will be done.\r\n\r\n```python\r\nfrom hpsklearn import hyperopt_estimator, pca\r\n\r\nestim = hyperopt_estimator( preprocessing=[ pca('my_pca') ] )\r\n```\r\n\r\nPreprocessing steps from sklearn that are built-in so far:\r\n\r\n* PCA\r\n* TfidfVectorizer\r\n* StandardScalar\r\n* MinMaxScalar\r\n* Normalizer\r\n* OneHotEncoder\r\n\r\nMore to come!\r\n\r\n## Installation\r\n\r\n```\r\ngit clone https://github.com/hyperopt/hyperopt-sklearn.git\r\ncd hyperopt\r\npip install -e .\r\n```\r\n\r\n## Documentation\r\n\r\nDocumentation coming soon. \r\n\r\nThis project is built upon:\r\n\r\n* [hyperopt](http://hyperopt.github.io/hyperopt/)\r\n* [scikit-learn](http://scikit-learn.org/)\r\n\r\n## Examples\r\n\r\nAn example on the MNIST digit data\r\n\r\n```python\r\nfrom hpsklearn import hyperopt_estimator, any_classifier\r\nfrom sklearn.datasets import fetch_mldata\r\nfrom hyperopt import tpe\r\nimport numpy as np\r\n\r\n# Download the data and split into training and test sets\r\n\r\ndigits = fetch_mldata('MNIST original')\r\n\r\nX = digits.data\r\ny = digits.target\r\n\r\ntest_size = int( 0.2 * len( y ) )\r\nnp.random.seed( seed )\r\nindices = np.random.permutation(len(X))\r\nX_train = X[ indices[:-test_size]]\r\ny_train = y[ indices[:-test_size]]\r\nX_test = X[ indices[-test_size:]]\r\ny_test = y[ indices[-test_size:]]\r\n\r\nestim = hyperopt_estimator( classifier=any_classifier('clf'),  \r\n                            algo=tpe.suggest, trial_timeout=300)\r\n\r\nestim.fit( X_train, y_train )\r\n\r\nprint( estim.score( X_test, y_test ) )\r\n# <<show score here>>\r\nprint( estim.best_model() )\r\n# <<show model here>>\r\n```\r\n\r\nNot all classifiers within sklearn support sparse data. To make life easier, hpsklearn comes with an any_sparse_classifier which will only sample from the classifiers available which accept sparse data.\r\n\r\n\r\n```python\r\nfrom hpsklearn import hyperopt_estimator, any_sparse_classifier, tfidf\r\nfrom sklearn.datasets import fetch_20newsgroups\r\nfrom sklearn import metrics\r\nfrom hyperopt import tpe\r\nimport numpy as np\r\n\r\n# Download the data and split into training and test sets\r\n\r\ntrain = fetch_20newsgroups( subset='train' )\r\ntest = fetch_20newsgroups( subset='test' )\r\nX_train = train.data\r\ny_train = train.target\r\nX_test = test.data\r\ny_test = test.target\r\n\r\nestim = hyperopt_estimator( classifier=any_sparse_classifier('clf'), \r\n                            preprocessing=[tfidf('tfidf')],\r\n                            algo=tpe.suggest, trial_timeout=300)\r\n\r\nestim.fit( X_train, y_train )\r\n\r\nprint( estim.score( X_test, y_test ) )\r\n# <<show score here>>\r\nprint( estim.best_model() )\r\n# <<show model here>>\r\n```","google":"","note":"Don't delete this file! It's used internally to help with page regeneration."}