
% ~/cvs/bardeberg/NIPS2011/jb.bib
% ~/cvs/bardeberg/NIPS2011/hyperoptim.bib

@misc{deep-learning-tutorials-hmc,
    author={G. Desjardins and J. Bergstra and {LISA Lab}},
    title={Deep Learning Tutorial: Hybrid {M}onte-{C}arlo},
    howpublished={http://www.deeplearning.net/tutorial/hmc.html},
    year={2010},
}
@Misc{scipy,
    author =    {E. Jones and T. Oliphant and P. Peterson and others},
    title =     {{SciPy}: Open source scientific tools for {Python}},
    year =      {2001--},
    howpublished = "http://www.scipy.org/"
}
@misc{20newsgroups,
    author={T. Mitchell},
    title={20 Newsgroups Data Set},
    howpublished={http://qwone.com/~jason/20Newsgroups/},
    year=1996
}

@InProceedings{BaKe10,
  author =       {Bardenet, R. and K\'{e}gl, B.},
  title =        {Surrogating the surrogate: accelerating {G}aussian {P}rocess
                  optimization with mixtures}, 
  booktitle =    {ICML}, 
  year =         {2010},
}

@article{bengio:2000,
    author = {Y. Bengio},
    title = {Gradient-based Optimization of Hyperparameters},
    year = {2000},
    journal = {Neural Computation},
    volume = 12,
    number = 8,
    pages = {1889-1900},
    annote = {}
}

@article{bergstra+bengio:2012,
    author = {J. Bergstra and Y. Bengio},
    title = {Random Search for Hyper-parameter Optimization},
    journal = {Journal of Machine Learning Research},
    year={2012},
    volume={13},
    pages={281--305},
}

@inproceedings{bergstra+bardenet+bengio+kegl:2011,
    author = {Bergstra, J. and Bardenet, R. and Bengio, Y. and K{\'e}gl, B.},
    title = {Algorithms for Hyper-parameter Optimization},
    pages={2546--2554},
    booktitle={NIPS*24},
    year={2011},
}

@inproceedings{bergstra+etal:2010,
     author = {Bergstra, J. and Breuleux, O. and Bastien, F. and Lamblin, P. and Pascanu, R. and Desjardins, G. and Turian, J. and Bengio, Y.},
      month = jun,
      title = {Theano: a {CPU} and {GPU} Math Expression Compiler},
  booktitle = {Proceedings of the {Python} for Scientific Computing Conference ({SciPy})},
       year = {2010},
   location = {Austin, TX},
}

@inproceedings{bergstra+pinto+cox:2012,
    author={J. Bergstra and N. Pinto and D. D. Cox},
    title={Machine Learning for Predictive Auto-Tuning with Boosted Regression Trees},
    booktitle={INPAR},
    year=2012,
}

@inproceedings{bergstra+yamins+cox:2013,
    author = {J. Bergstra and D. Yamins and D. D. Cox},
    title = {Making a Science of Model Search: Hyperparameter Optimization in Hundreds of Dimensions for Vision Architectures},
    booktitle = {In Proc. ICML},
    year={2013},
    volume={},
    pages={},
}


@phdthesis{brochu:2010,
    author={E. Brochu},
    title={Interactive {B}ayesian Optimization: Learning Parameters for Graphics and Animation},
    school={University of British Columbia},
    month=dec,
    year=2010,
}

@inproceedings{coates+lee+ng:2011,
    title={An Analysis of Single-Layer Networks in Unsupervised Feature Learning},
    author={A. Coates and H. Lee and A. Y. Ng},
    booktitle={Proc. AISTATS-14},
    year={2011},
}

@inproceedings{coates+ng:2011,
    title={The Importance of Encoding Versus Training with Sparse Coding and Vector Quantization},
    author={A. Coates and A. Y. Ng},
    year={2011},
    booktitle={Proc. ICML-28},
}

@article{dicarlo+zoccolan+rust:2012,
    title = {How does the brain solve visual object recognition?},
    journal = {Neuron},
    volume = {73},
    year = {2012},
    month = {2012 Feb 9},
    pages = {415-34},
    abstract = {Mounting evidence suggests that {\textquoteright}core object recognition,{\textquoteright} the ability to rapidly recognize objects despite substantial appearance variation, is solved in the brain via a cascade of reflexive, largely feedforward computations that culminate in a powerful neuronal representation in the inferior temporal cortex. However, the algorithm that produces this solution remains poorly understood. Here we review evidence ranging from~individual neurons and neuronal populations to behavior and computational models. We propose that understanding this algorithm will require using neuronal and psychophysical data to sift through many computational models, each based on building blocks of small, canonical subnetworks with a common functional goal.},
    issn = {1097-4199},
    author = {J. J. {DiCarlo} and D. Zoccolan and N. C. Rust}
}

@INPROCEEDINGS{swersky+duvenaud+snoek+hutter+osborne:2013,
    AUTHOR =       {K. Swersky and D. Duvenaud and J. Snoek and F. Hutter and M. Osborne},
    TITLE =        {Raiders of the Lost Architecture: Kernels for Bayesian Optimization in Conditional Parameter Spaces},
    BOOKTITLE =    {NIPS workshop on Bayesian Optimization in Theory and Practice},
    YEAR =         {2013},
    MONTH =        {10~} # dec,
}

@INPROCEEDINGS{eggensperger+etal:2013,
    AUTHOR =       {K. Eggensperger and M. Feurer and F. Hutter and J. Bergstra and J. Snoek and H. Hoos and K. Leyton-Brown},
    TITLE =        {Towards an Empirical Foundation for Assessing Bayesian Optimization of Hyperparameters},
    BOOKTITLE =    {NIPS workshop on Bayesian Optimization in Theory and Practice},
    YEAR =         {2013},
    MONTH =        {10~} # dec,
}

@article{fan+chang+hsieh+wang+lin:2008,
    author={R.-E. Fan and K.-W. Chang and C.-J. Hsieh and X.-R. Wang and and C.-J. Lin},
    title={LIBLINEAR: A library for large linear classification},
    journal={Journal of Machine Learning Research},
    volume=9,
    year=2008,
    pages={1871-1874},
}

@article{fukushima:1980,
    author={K. Fukushima},
    title={Neocognitron: A Self-Organizing Neural Network Model for a Mechanism of Pattern Recognition unaffected by Shift in Position},
    journal={Biological Cybernetics},
    volume={36},
    number={4},
    pages={193--202},
    year={1980},
}

@inproceedings{guan2009class,
     title={A class-feature-centroid classifier for text categorization},
     author={Guan, Hu and Zhou, Jingyu and Guo, Minyi},
	      booktitle={Proceedings of the 18th international conference on World wide web},
	        pages={201--210},
		  year={2009},
		    organization={ACM}
}

@article{hinton:2002,
    author={G. E. Hinton},
    title={Training Products of Experts by Minimizing Contrastive Divergence},
    journal={Neural Computation},
    volume=14,
    pages={1771-1800},
    year=2002,
    annote={Introduces Contrastive Divergence (CD) for training
        product-of-experts models such as the RBM.}
}

@article{hinton+osindero+teh:2006,
    author={G. E. Hinton and S. Osindero and Y. Teh},
    title={A Fast Learning Algorithm for Deep Belief Nets},
    year=2006,
    journal={Neural Computation},
    volume=18,
    pages={1527-1554},
    annote={Stacking RBMs trained by Contrastive Divergence, pre-training.}
}

@techreport{huang+ramesh+berg+learnedmiller:2007,
    author={G. B. Huang and M. Ramesh and T. Berg and E. Learned-Miller},
    title={Labeled Faces in the Wild: A Database for Studying Face Recognition in Unconstrained Environments},
    institution={University of Massachusetts, Amherst},
    number={07-49},
    month=oct,
    year=2007,
}

@phdthesis{hutter:2009,
    author={F. Hutter},
    title={Automated Configuration of Algorithms for Solving Hard Computational
        Problems},
    year=2009,
    school={University of British Columbia},
    annote={Evaluates model-based and model-free approaches to program
        configuration.  This work is relevant to the tuning of learning
            algorithm hyper-parameters.}
}

@inproceedings{hutter+hoos+leyton-brown:2011,
    author={F. Hutter and H. Hoos and K. Leyton-Brown},
    title={Sequential Model-Based Optimization for General Algorithm Configuration},
    booktitle={LION-5},
    note={Extended version as UBC Tech report TR-2010-10.},
    year={2011},
}

@article{hyvarinen+oja:2000,
    author={A. Hyv{\"a}rinen and E. Oja},
    title={Independent Component Analysis: Algorithms and
        Applications},
    journal={Neural Networks},
    volume=13,
    number={4--5},
    pages={411--430},
    year=2000,
    annote={ICA tutorial. The difference between PCA and ICA is that ICA
        maximizes the marginal non-gaussian-ness of each latent variable. The
            assumption is that when two non-gaussian latent variables (the
                    sources) are combined linearly (by mixing) then the result
            is close to Gaussian than either of the original sources. ICA seeks
    to undo this mixing by maximizing the un-gaussian-ness of each latent
    variable.  When the sources are indeed scalars being combined linearly, it
    seems like a good idea and algorithm. ICA is not a particular algorithm.
    Different techniques estimate non-Gaussian-ness, such as maximizing
    abs or square of kurtosis for given variance, and minimizing entropy.
    Several criteria are listed. This tutorial also describes the EVD-based
    whitening procedure (aka ZCA) in Section 5.2. Section 6 presents the FastICA
    algorithm.}
}

@Article{Jon01,
  author =       {Jones, D.R.},
  title =        {A Taxonomy of Global Optimization Methods 
                  Based on Response Surfaces},
  journal =      {Journal of Global Optimization},
  pages =        {345--383},
  volume =       {21},
  year =         {2001}
}

@inproceedings{kivinen+warmuth:1999,
    author = {J. Kivinen and M. Warmuth},
    title = {Boosting as Entropy Projection},
    booktitle = {COLT},
    year={1999},
    annote = {Introduces TotalBoost}
}

@techreport{krizhevsky:2009,
    title={Learning Multiple Layers of Features from Tiny Images},
    author={A. Krizhevsky},
    year={2009},
    annote={GRBM results on {CIFAR}-10},
    institution={University of Toronto},
}

@article{lecun+etal:1989,
    author={Y. {LeCun} and B. Boser and J. S. Denker and D. Henderson and R. E.
        Howard and W. Hubbard and L. D. Jackel},
    title={Backpropagation Applied to Handwritten Zip Code Recognition},
    journal={Neural Computation},
    volume=1,
    number=4,
    pages={541-551},
    year=1989,
    annote={Introduces convolutional neural network.}
}
@article{lecun+bottou+bengio+haffner:1998,
    author =    {{LeCun}, Y. and Bottou, L. and Bengio, Y. and Haffner, P.},
    title =     {Gradient-Based Learning Applied to Document Recognition},
    journal =   {Proceedings of the IEEE},
    month =         {November},
    volume =        86,
    number =        11,
    pages =  {2278--2324},
    year =      1998,
    annote = {This is the standard citation for MNIST and the LeNet5 architecture.}
}

@inproceedings{lowe:1999,
    author={D. G. Lowe},
    year=1999,
    title={Object Recognition from Local Scale-Invariant Features},
    booktitle={Proceedings of the International Conference on Computer
        Vision 2 ({ICCV})},
    pages={1150--1157},
    doi={10.1109/ICCV.1999.790410},
    annote={Describes SIFT feature.}
}

@Incollection{MoTiZi78,
  author =       {Mockus, J. and Tiesis, V. and Zilinskas, A.},
  title =        {The application of {B}ayesian methods for seeking the extremum},
  editor =       {Dixon, L.C.W. and Szego, G.P.},
  booktitle =    {Towards Global Optimization},
  publisher =    {North Holland, New York},
  volume =       {2},
  pages =        {117--129},
  year =         {1978}
}

@incollection{SVHN:2011,
    author={Netzer, Y. and Coates, A. and Bissacco, A. and Wu, B. and Ng, A. Y.},
    title={Reading Digits in Natural Images with Unsupervised Feature Learning},
    booktitle={{NIPS} Workshop on Deep Learning and Unsupervised Feature Learning},
    year={2011},
}

@inproceedings{pinto+cox:2011,
    author={Pinto, N. and Cox, D. D.},
    title={Beyond Simple Features: A Large-Scale Feature Search Approach to Unconstrained Face Recognition},
    booktitle={Proc. Face and Gesture Recognition},
    year={2011},
}

@article{pinto+cox+dicarlo:2008,
    author={N. Pinto and D. D. Cox and J. J. {DiCarlo}},
    title={Why is Real-World Visual Object Recognition Hard?},
    journal={PLoS Computational Biology},
    volume=4,
    number=1,
    doi={doi:10.1371/journal.pcbi.0040027},
    year={2008},
    annote={Presents a classification model that is V1-like elements with a
        linear SVM classifier on top.  This model achieves results on
            Caltech-101 and Caltech-256, but fails to recognize objects that
            have been super-imposed on a white-noise background.  The conclusion
            is that backgrounds in photographs are being used by the model.
    The paper includes instructions for how to implement a V1-like
    hidden layer of simple cells, including contrast gain-normalization in both
    the input (retinal ganglion) and output (simple cell response).
    This paper points out the problem of sample bias in using un-controlled
    data for high-dimensional problems with many factors of variation.  It is
    not possible to gather enough data to make systematic tests.  These
    difficulties are very similar to the motivation behind the BabyAI-related
    projects in our lab.}
}

@inproceedings{pinto+stone+zickler+cox:2011,
    author={Pinto, N. and Stone, Z. and Zickler, T. and Cox, D. D.},
    title={{Scaling-up Biologically-Inspired Computer Vision: A Case-Study on Facebook}},
    booktitle={IEEE Computer Vision and Pattern Recognition, Workshop on Biologically Consistent Vision},
    year={2011}
}

@article{pinto+doukhan+dicarlo+cox:2009,
  author = {Pinto, N. and Doukhan, D. and {DiCarlo}, J. J. and Cox, D. D.},
  journal = {PLoS Comput Biol},
  publisher = {Public Library of Science},
  title = {A High-Throughput Screening Approach to Discovering Good Forms of Biologically Inspired Visual Representation},
  year = {2009},
  month = {11},
  volume = {5},
  pages = {e1000579},
  number = {11},
}
@article{polikar:2009,
    author = {R. Polikar},
    title = {Ensemble Learning},
    journal = {Scholarpedia},
    volume = 4,
    number = 1,
    pages = {2776},
}

@techreport{neal:1993,
    author={Neal, R. M.},
    year={1993},
    title={Probabilistic Inference Using {M}arkov Chain {M}onte {C}arlo Methods},
    institution={Dept. of Computer Science, University of Toronto},
    number={CRG-TR-93-1},
}

@book{rasmussen+williams:2006,
    author={C. E. Rasmussen and C. K. I. Williams},
    title={Gaussian Processes for Machine Learning},
    year=2006,
    publisher={{MIT} Press},
    path={reading/2006/RW.pdf},
    annote={Gaussian process textbook.}
}

@article{riesenhuber+poggio:1999,
    author={M. Riesenhuber and T. Poggio},
    title={Hierarchical Models of Object Recognition in Cortex},
    journal={Nature Neuroscience},
    year={1999},
    volume=2,
    pages={1019--1025},
    annote={Describes HMAX model.}
}

@article{sklearn,
    title={{Scikit-learn: Machine Learning in {Python}}},
    author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
       and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
                    and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
                             Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
    journal={Journal of Machine Learning Research},
    volume={12},
    pages={2825--2830},
    year={2011}
}

@misc{snoek+larochelle+adams:2011,
    author={J. Snoek and H. Larochelle and R. P. Adams},
    title={Opportunity Cost in {B}ayesian Optimization},
    howpublished={NIPS Workshop on {B}ayesian Optimization, Experimental Design and Bandits},
    year={2011},
}
@inproceedings{snoek+larochelle+adams:2012nips,
    author = {Snoek, J. and Larochelle, H. and Adams, R. P.},
    booktitle = {Neural Information Processing Systems},
    year = {2012},
    title = {Practical {B}ayesian Optimization of Machine Learning Algorithms},
}
@article{thornton+hutter+hoos+leyton-brown:2012,
    author    = {C. Thornton and F. Hutter and H. H. Hoos and K. Leyton-Brown},
    title     = {Auto-{WEKA}: Automated Selection and Hyper-Parameter Optimization
    of Classification Algorithms},
    journal   = {CoRR},
    volume    = {abs/1208.3719},
    year      = {2012},
    ee        = {http://arxiv.org/abs/1208.3719},
    bibsource = {DBLP, http://dblp.uni-trier.de}
}
@inproceedings{warmuth+liao+ratsch:2006,
    author = {M. Warmuth and J. Liao and G. {R{\"a}tsch}},
    title={Totally Corrective Boosting Algorithms that Maximize the Margin},
    year={2006},
    booktitle={ICML},
    pages={1001-1008},
    annote={Proof for something about TotalBoost_v}
}
@article{demiriz+bennett+shawe-taylor:2002,
    author={A. Demiriz and K.P. Bennett and J. Shawe-Taylor},
    title={Linear Programming Boosting via Column Generation},
    journal={Machine Learning},
    year={2002},
    volume=46,
    pages={225--254}
}

@misc{hyperopt-convnet,
    author={J. Bergstra and D. Yamins and N. Pinto},
    title={Hyperparameter Optimization for Convolutional Vision Architectures},
    howpublished={https://github.com/jaberg/hyperopt-convnet},
    year=2013
}
@inproceedings{hyperopt,
    author={J. Bergstra and D. Yamins and D. D. Cox},
    title={Hyperopt: A {P}ython Library for Optimizing the Hyperparameters of
        Machine Learning Algorithms},
    booktitle={SciPy'13},
    year=2013
}
@article{hall2009weka,
    title={The WEKA data mining software: an update},
    author={Hall, M. and Frank, E. and Holmes, G. and Pfahringer, B. and Reutemann, P. and Witten, I. H.},
    journal={ACM SIGKDD explorations newsletter},
    volume={11},
    number={1},
    pages={10--18},
    year={2009},
    publisher={ACM}
}
@article{sill+takacs+mackey+lin:2009,
    author    = {J. Sill and G. Tak{\'a}cs and L. Mackey and D. Lin},
    title     = {Feature-Weighted Linear Stacking},
    journal   = {CoRR},
    volume    = {abs/0911.0460},
    year      = {2009},
    ee        = {http://arxiv.org/abs/0911.0460},
    bibsource = {DBLP, http://dblp.uni-trier.de}
}
@article{wolpert:1992,
    author = {D. H. Wolpert},
    title = {Stacked Generalization},
    journal = {Neural Networks},
    volume = {5},
    pages = {241--259},
    year = {1992},
    annote = {Introduces Stacking for weighting ensemble members}
}
@article{breiman:1996stacked,
    author = {L. Breiman},
    title = {Stacked Regressions},
    journal = {Machine Learning},
    volume = {24},
    pages = {49--64},
    year = {1996}
}
@article{breiman:1996bagging,
    author = {L. Breiman},
    title = {Bagging Predictors},
    journal = {Machine Learning},
    volume = {24},
    pages = {123--140},
    year = {1996}
}
@article{hoeting+madigan+raftery+volinsky:1999,
    title={Bayesian Model Averaging: A Tutorial},
    author={Hoeting, J. A. and Madigan, D. and Raftery, A. E. and Volinsky, C. T},
    journal={Statistical Science},
    pages={382--401},
    year={1999},
    publisher={JSTOR}
}
@article{clarke:2003,
    author = {B. Clarke},
    title = {Bayes model averaging and stacking when model approximation error cannot be ignored},
    journal = {Machine Learning Research},
    pages={683--712},
    year={2003},
}
@InCollection{Rumelhart86c,
  author =       "D. E. Rumelhart and G. E. Hinton and R. J. Williams",
  editor =       "D. E. Rumelhart and J. L. McClelland",
  booktitle =    "Parallel Distributed Processing",
  title =        "Learning Internal Representations by Error
                 Propagation",
  chapter =      "8",
  volume =       "1",
  publisher =    "MIT Press",
  address =      "Cambridge",
  pages =        "318--362",
  year =         "1986",
}

@article{goodfellow+warde-farley+mirza+courville+bengio:2013,
    author    = {I. Goodfellow and D. Warde-Farley and M. Mirza and A. Courville and Y. Bengio},
    title     = {Maxout Networks},
    journal   = {CoRR},
    volume    = {abs/1302.4398},
    year      = {2013},
}

@article{hinton+srivastava+krizhevsky+sutskever+salakhutdinov:2012,
    author    = {G. E. Hinton and N. Srivastava and A. Krizhevsky and I. Sutskever and R. R. Salakhutdinov},
    title     = {Improving Neural Networks by Preventing Co-Adaptation of Feature Detectors},
    journal   = {CoRR},
    volume    = {abs/1207.0580},
    year      = {2012},
}
@INPROCEEDINGS{larochelle+etal:2007,
    author = {Larochelle, H. and Erhan, D. and Courville, A. and Bergstra, J. and Bengio, Y.},
     title = {An Empirical Evaluation of Deep Architectures on Problems with Many Factors of Variation},
     pages = {473--480},
     crossref = {ICML:2007},
}
@proceedings{ICML:2007,
  year = {2007},
  location = {Corvallis, OR},
  title = "{ICML}",
  booktitle = {ICML},
  editor   = {},
  publisher= {},
}

@inproceedings{vincent+larochelle+bengio+manzagol:2008,
    title={Extracting and Composing Robust Features with Denoising Autoencoders},
    author={Vincent, P. and Larochelle, H. and Bengio, Y. and Manzagol, P-A.},
    booktitle={Proc. of the 25th ICML},
    pages={1096--1103},
    year={2008},
    organization={ACM}
}
@article{zhu+byrd+nocedal:1997,
    author={Zhu, C. and R. H. Byrd and J. Nocedal},
    year={1997},
    title={{L-BFGS-B}: Algorithm 778: {L-BFGS-B}, {FORTRAN} routines for large scale bound constrained optimization},
    journal={{ACM} Transactions on Mathematical Software},
    volume=23,
    number=4,
    pages={550-560},
}
